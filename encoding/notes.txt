Unicode, formally the Unicode Standard, is an information technology standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. 

Unicode can be implemented by different character encodings?
The Unicode standard defines Unicode Transformation Formats (UTF)
UTF-8, the dominant encoding on the World Wide Web (used in over 95% of websites as of 2020, and up to 100% for some languages)[4] and on most Unix-like operating systems, uses one byte (8 bits) for the first 128 code points, and up to 4 bytes for other characters
The first 128 Unicode code points represent the ASCII characters, which means that any ASCII text is also a UTF-8 text.
Grapheme - combined character sequences

Unicode takes the role of providing a unique code point—a number, not a glyph—for each character
In other words, Unicode represents a character in an abstract way and leaves the visual rendering (size, shape, font, or style) to other software, such as a web browser or word processor. 
The Unicode Standard defines a codespace,[59] a set of numerical values ranging from 0 through 10FFFF16,[60] called code points[61] and denoted as U+0000 through U+10FFFF

Input methods
=============
Basic method - a beginning sequence is followed by the hexadecimal representation of the code point and the ending sequence.
screen-selection entry method 


Hit numlock
Must convert hex codepoint to decimal
ALT142 -> Ä
ALT9385 -> ®
ALT10014 -> ▲
ALT7545 -> y
ALT0013 -> Carriage Return
ALT0009 -> Tab
ALT0010 -> Line feed
